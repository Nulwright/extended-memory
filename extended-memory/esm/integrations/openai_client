```python
"""OpenAI API Integration"""

import asyncio
import logging
import openai
from typing import List, Optional, Dict, Any
from openai.error import OpenAIError, RateLimitError, InvalidRequestError

from esm.config import get_settings
from esm.utils.exceptions import EmbeddingError, ConfigurationError, ServiceUnavailableError

logger = logging.getLogger(__name__)


class OpenAIClient:
    """Client for OpenAI API integration"""
    
    def __init__(self):
        self.settings = get_settings()
        self.client = None
        self._initialize_client()
    
    def _initialize_client(self):
        """Initialize OpenAI client"""
        try:
            if not self.settings.openai_api_key:
                logger.warning("OpenAI API key not provided")
                return
            
            openai.api_key = self.settings.openai_api_key
            self.client = openai
            
            logger.info("OpenAI client initialized")
            
        except Exception as e:
            logger.error(f"Failed to initialize OpenAI client: {e}")
            raise ConfigurationError(f"OpenAI client initialization failed: {e}")
    
    async def generate_embedding(
        self, 
        text: str, 
        model: str = "text-embedding-ada-002"
    ) -> Optional[List[float]]:
        """Generate embedding for text"""
        try:
            if not self.client:
                logger.warning("OpenAI client not available")
                return None
            
            if not text or len(text.strip()) == 0:
                logger.warning("Empty text provided for embedding")
                return None
            
            # Truncate text if too long
            if len(text) > 8000:
                text = text[:8000]
                logger.warning("Text truncated for embedding generation")
            
            # Generate embedding
            response = await asyncio.to_thread(
                self.client.Embedding.create,
                input=text,
                model=model
            )
            
            if response and response.data:
                embedding = response.data[0].embedding
                logger.debug(f"Generated embedding with {len(embedding)} dimensions")
                return embedding
            
            logger.error("No embedding data in OpenAI response")
            return None
            
        except RateLimitError as e:
            logger.error(f"OpenAI rate limit exceeded: {e}")
            raise ServiceUnavailableError("OpenAI", "Rate limit exceeded")
        
        except InvalidRequestError as e:
            logger.error(f"Invalid OpenAI request: {e}")
            raise EmbeddingError(f"Invalid request: {e}")
        
        except OpenAIError as e:
            logger.error(f"OpenAI API error: {e}")
            raise EmbeddingError(f"OpenAI API error: {e}")
        
        except Exception as e:
            logger.error(f"Unexpected error generating embedding: {e}")
            raise EmbeddingError(f"Embedding generation failed: {e}")
    
    async def generate_batch_embeddings(
        self, 
        texts: List[str], 
        model: str = "text-embedding-ada-002",
        batch_size: int = 100
    ) -> List[Optional[List[float]]]:
        """Generate embeddings for multiple texts"""
        try:
            if not self.client:
                logger.warning("OpenAI client not available")
                return [None] * len(texts)
            
            results = []
            
            # Process in batches to respect rate limits
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]
                
                # Clean and truncate texts
                processed_batch = []
                for text in batch:
                    if text and len(text.strip()) > 0:
                        processed_text = text[:8000] if len(text) > 8000 else text
                        processed_batch.append(processed_text)
                    else:
                        processed_batch.append("")
                
                try:
                    # Generate embeddings for batch
                    response = await asyncio.to_thread(
                        self.client.Embedding.create,
                        input=processed_batch,
                        model=model
                    )
                    
                    if response and response.data:
                        batch_embeddings = [item.embedding for item in response.data]
                        results.extend(batch_embeddings)
                        logger.debug(f"Generated {len(batch_embeddings)} embeddings in batch")
                    else:
                        logger.error("No embedding data in batch response")
                        results.extend([None] * len(batch))
                    
                    # Small delay between batches to respect rate limits
                    if i + batch_size < len(texts):
                        await asyncio.sleep(0.1)
                        
                except (RateLimitError, OpenAIError) as e:
                    logger.error(f"Batch embedding failed: {e}")
                    results.extend([None] * len(batch))
                    
                    # Longer delay on error
                    await asyncio.sleep(1.0)
            
            return results
            
        except Exception as e:
            logger.error(f"Batch embedding generation failed: {e}")
            return [None] * len(texts)
    
    async def generate_completion(
        self,
        prompt: str,
        model: str = "gpt-3.5-turbo",
        max_tokens: int = 150,
        temperature: float = 0.7
    ) -> Optional[str]:
        """Generate text completion"""
        try:
            if not self.client:
                logger.warning("OpenAI client not available")
                return None
            
            # Use chat completion for modern models
            if model.startswith("gpt-3.5") or model.startswith("gpt-4"):
                response = await asyncio.to_thread(
                    self.client.ChatCompletion.create,
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                
                if response and response.choices:
                    return response.choices[0].message.content.strip()
            else:
                # Use legacy completion endpoint for older models
                response = await asyncio.to_thread(
                    self.client.Completion.create,
                    model=model,
                    prompt=prompt,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                
                if response and response.choices:
                    return response.choices[0].text.strip()
            
            return None
            
        except (RateLimitError, InvalidRequestError, OpenAIError) as e:
            logger.error(f"OpenAI completion failed: {e}")
            return None
        except Exception as e:
            logger.error(f"Unexpected error in completion: {e}")
            return None
    
    async def moderate_content(self, text: str) -> Dict[str, Any]:
        """Check content for policy violations"""
        try:
            if not self.client:
                return {"flagged": False, "error": "Client not available"}
            
            response = await asyncio.to_thread(
                self.client.Moderation.create,
                input=text
            )
            
            if response and response.results:
                result = response.results[0]
                return {
                    "flagged": result.flagged,
                    "categories": result.categories,
                    "category_scores": result.category_scores
                }
            
            return {"flagged": False}
            
        except Exception as e:
            logger.error(f"Content moderation failed: {e}")
            return {"flagged": False, "error": str(e)}
    
    async def health_check(self) -> Dict[str, Any]:
        """Check OpenAI service health"""
        try:
            if not self.client:
                return {
                    "status": "unavailable",
                    "error": "Client not initialized",
                    "api_key_configured": bool(self.settings.openai_api_key)
                }
            
            # Test with a simple embedding request
            test_embedding = await self.generate_embedding("test")
            
            if test_embedding:
                return {
                    "status": "healthy",
                    "service": "openai",
                    "embedding_model": "text-embedding-ada-002",
                    "embedding_dimensions": len(test_embedding),
                    "api_key_configured": True
                }
            else:
                return {
                    "status": "unhealthy",
                    "error": "Failed to generate test embedding",
                    "service": "openai",
                    "api_key_configured": bool(self.settings.openai_api_key)
                }
                
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "service": "openai",
                "api_key_configured": bool(self.settings.openai_api_key)
            }
    
    async def get_model_info(self, model_name: str) -> Optional[Dict[str, Any]]:
        """Get information about a specific model"""
        try:
            if not self.client:
                return None
            
            models = await asyncio.to_thread(self.client.Model.list)
            
            for model in models.data:
                if model.id == model_name:
                    return {
                        "id": model.id,
                        "object": model.object,
                        "owned_by": model.owned_by,
                        "created": model.created
                    }
            
            return None
            
        except Exception as e:
            logger.error(f"Failed to get model info for '{model_name}': {e}")
            return None
    
    async def list_available_models(self) -> List[str]:
        """List available models"""
        try:
            if not self.client:
                return []
            
            models = await asyncio.to_thread(self.client.Model.list)
            return [model.id for model in models.data]
            
        except Exception as e:
            logger.error(f"Failed to list available models: {e}")
            return []
